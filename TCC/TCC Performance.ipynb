{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0LUX0LaYmKNa"
   },
   "source": [
    "<h1 align='center'>Fake News Detection</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4VulXquoOxhD"
   },
   "source": [
    "<h2>Importing Libraries And Datasets</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 944
    },
    "colab_type": "code",
    "id": "bDfO3ppL9TVX",
    "outputId": "6148599e-04ca-41fe-8ef3-2eb8a0c70a73"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from tensorflow.keras.preprocessing.text import one_hot, Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Embedding, Input, LSTM, Conv1D, MaxPool1D, Bidirectional\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8QiTczEunJNx",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loading The Data\n",
    "df_true = pd.read_csv(\"True.csv\")\n",
    "df_fake = pd.read_csv(\"Fake.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iBqxz1TBPBLE"
   },
   "source": [
    "<h2>Performing Exploratory Data Analysis</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "2IxC_95sovyJ",
    "outputId": "59b36685-ff10-460a-8e3a-86ed88e96f9f",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Adding A Target Class Column To Indicate Whether The News Is Real Or Fake\n",
    "df_true['isfake'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "Ll5weq9qpGQO",
    "outputId": "1b5fbf06-78be-475f-f56d-6028efedf202",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_fake['isfake'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "colab_type": "code",
    "id": "PgTEJ5aGpHfU",
    "outputId": "86b662e9-b198-4132-d209-3af7e714fd6a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Concatenating Real And Fake News\n",
    "df = pd.concat([df_true, df_fake]).reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jgkZHO3CPnsp"
   },
   "source": [
    "<h2>Performing Data Cleaning</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining Additional Stopwords From nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('portuguese')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Stopwords And Remove Words With 2 Or Less Characters\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3 and token not in stop_words:\n",
    "            result.append(token)\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying The Function To The Dataframe\n",
    "df['clean'] = df['text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining The Total Words Present In The Dataset\n",
    "list_of_words = []\n",
    "for i in df.clean:\n",
    "    for j in i:\n",
    "        list_of_words.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2388359"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76697"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtaining The Total Number Of Unique Words\n",
    "total_words = len(list(set(list_of_words)))\n",
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NKfwnpbC6-i2"
   },
   "outputs": [],
   "source": [
    "# Joining The Words Into A String\n",
    "df['clean_joined'] = df['clean'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "r20ny06ECP1B",
    "outputId": "04ffbdd8-f2e7-4b62-a84a-569c033dca87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum number of words in any document is = 3904\n"
     ]
    }
   ],
   "source": [
    "# Determining The Maximum Number Of Words In Any Document Required To Create Word Embeddings \n",
    "maxlen = -1\n",
    "for doc in df.clean_joined:\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    if(maxlen<len(tokens)):\n",
    "        maxlen = len(tokens)\n",
    "print(\"The maximum number of words in any document is =\", maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zi3neznbP_QT"
   },
   "source": [
    "<h2>Preparing The Data By Performing Tokenization And Padding</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lUgEf-SZ7R7c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Splitting Data Into Test And Train \n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(df.clean_joined, df.isfake, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vwOEitoc9TWs"
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kme-IYsM6uJa"
   },
   "outputs": [],
   "source": [
    "# Creating A Tokenizer To Tokenize The Words And Create Sequences Of Tokenized Words\n",
    "tokenizer = Tokenizer(num_words = total_words)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "train_sequences = tokenizer.texts_to_sequences(x_train)\n",
    "test_sequences = tokenizer.texts_to_sequences(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "URy4Wkai_Qh3"
   },
   "outputs": [],
   "source": [
    "# Adding Padding\n",
    "padded_train = pad_sequences(train_sequences,maxlen = 40, padding = 'post', truncating = 'post')\n",
    "padded_test = pad_sequences(test_sequences,maxlen = 40, truncating = 'post') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MHckYpNqQaMf"
   },
   "source": [
    "<h2>Building And Training The Model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "Z0d7qNMLUCc3",
    "outputId": "ffe8ab5d-055c-47ef-a388-116c29771574"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_8 (Embedding)     (None, None, 128)         9817216   \n",
      "                                                                 \n",
      " bidirectional_8 (Bidirectio  (None, 256)              263168    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,113,409\n",
      "Trainable params: 10,113,409\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Sequential Model\n",
    "model = Sequential()\n",
    "\n",
    "# Embeddidng layer\n",
    "model.add(Embedding(total_words, output_dim = 128))\n",
    "\n",
    "\n",
    "# Bi-Directional LSTM\n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "\n",
    "# Dense layers\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dense(1,activation= 'sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gpgmJizW9TXC"
   },
   "outputs": [],
   "source": [
    "y_train = np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "colab_type": "code",
    "id": "SEdds6i0-ohn",
    "outputId": "e03fbe64-8b3e-44f3-e2de-910475ef759e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "162/162 [==============================] - 7s 27ms/step - loss: 0.4249 - acc: 0.7941 - val_loss: 0.3281 - val_acc: 0.8698\n",
      "Epoch 2/5\n",
      "162/162 [==============================] - 4s 24ms/step - loss: 0.0765 - acc: 0.9766 - val_loss: 0.3638 - val_acc: 0.8715\n",
      "Epoch 3/5\n",
      "162/162 [==============================] - 4s 24ms/step - loss: 0.0138 - acc: 0.9967 - val_loss: 0.6363 - val_acc: 0.8576\n",
      "Epoch 4/5\n",
      "162/162 [==============================] - 4s 24ms/step - loss: 0.0077 - acc: 0.9985 - val_loss: 0.5270 - val_acc: 0.8524\n",
      "Epoch 5/5\n",
      "162/162 [==============================] - 4s 23ms/step - loss: 6.1749e-04 - acc: 0.9998 - val_loss: 0.7872 - val_acc: 0.8576\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x191652f2c40>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model\n",
    "model.fit(padded_train, y_train, validation_split = 0.1, epochs = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RXa2MqEQQ2ww"
   },
   "source": [
    "<h2>Assessing Trained Model Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7qRUkys-BSuQ"
   },
   "outputs": [],
   "source": [
    "# Making prediction\n",
    "pred = model.predict(padded_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qh66RfZgF7ln"
   },
   "outputs": [],
   "source": [
    "# If The Predicted Value Is >0.95 (i.e., More Than 95%), It Is Real Else It Is Fake\n",
    "prediction = []\n",
    "for i in range(len(pred)):\n",
    "    if pred[i].item() > 0.95:\n",
    "        prediction.append(1)\n",
    "    else:\n",
    "        prediction.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xhwBJVQo9TXN",
    "outputId": "57bc6601-def8-4a2f-80f2-abb0c493aed1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy :  0.8458333333333333\n",
      "F1-Score :  0.8377192982456141\n"
     ]
    }
   ],
   "source": [
    "# Getting The Accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "accuracy = accuracy_score(list(y_test), prediction)\n",
    "f1 = f1_score(list(y_test), prediction)\n",
    "\n",
    "print(\"Model Accuracy : \", accuracy)\n",
    "print(\"F1-Score : \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.89      0.85       726\n",
      "           1       0.88      0.80      0.84       714\n",
      "\n",
      "    accuracy                           0.85      1440\n",
      "   macro avg       0.85      0.85      0.85      1440\n",
      "weighted avg       0.85      0.85      0.85      1440\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, prediction))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FakeNewsClassiication.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
